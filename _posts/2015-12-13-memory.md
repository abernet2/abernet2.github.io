---
layout: markdown
title: Computer Memory
image: <img src="../imgs/doc.svg" alt="article-thumbnail">
excerpt: Learn about Stacks, Heaps, Caches and more in this in-depth description of how computer memory works!
---

## Starting Out: A Broad Overview

A computer works a lot like a person. For the purposes of discussing memory, it has a brain that is very powerful, but only has a tiny amount of room to remember things it needs to focus on. To put it another way, a human might be smart enough to do long division or calculus, but it can be extremely taxing to do so in one's head. That's why when you do math you usually use scratch paper.

Computers work the same way: they need scratch paper. The CPU, and other pieces of hardware, have small bits of working memory that help them to do their work quickly and efficiently. These small bits of memory in layers of memory called caches. 

Caching is a broad term that is important for performance. Every computer has layers of caches between the hardrive and various features of RAM. They come in various sizes with various ease of access. For the purpose of this blog post, just think of cache as the memory in the CPU, but keep in mind there is more to the term!

There is also the third layer of memory, the harddrive. RAM is downright puny compared to the harddrive. Whereas caches and RAM disappear without power, the hard-drive remembers what you write to it without any power. It also can store much more data in much less space, although these advantages come at a huge speed disadvantage. That's why harddrives are often compared to file cabinets: they are slow and painful to use, but great for big, longterm storage.

Each layer of memory serves a distinct purpose. It would be frustrating to constantly have to go in and out of a file cabinent to do a math problem. Likewise it would be very difficult after a while to sort through the big stack of your old pieces of scratch paper for any meaningful data.

## Stacks

Stack is a word you'll hear a lot in conversations about computers. There's "Stack" Overflow, Full "Stack" Employees, Kernel Stacks, etc. You might be interested to learn that this is simply a basic data type in computer science and it is based (essentially) off a stack of papers on your desk.

A stack works like a stack of papers on your desk. When you add a paper to the top, we call that a *push* and when you take a paper off the top we call that a *pop*. Imagine you have 10 pieces of paper with a letter on each one, A-J. Now imagine pushing all 10 pieces to the stack. Now pop the top page off and try to guess what letter it is? The correct answer is the last paper you added, if you went in order that would be J. Pushing items in order reverses the order. This is what is meant by Last In First Out (LIFO), a common phrase associated with stacks.

Stacking prioritizes the most recent additions. Notice that if you want something in the middle of the stack, this is a pretty inconvenient way of storing stuff. In the case of computers executing complicated sets of instructions, this is actually really helpful.

Computers are always using stacks. If you hit alt-tab on windows, you can scroll through your most recently used programs, with the most recent first. This is most likely the result of some sort of stack. The back and forward buttons on your browser use a stack, pushd and popd on the command line use a stack and every program you write uses a stack to manage its memory.

### Cool, Tell Me More About Memory!

Computers are dumb, which is why we need to be smart and help them stay organized. Stacks are a great way for computers to organize their memory because it keeps recent accesses in close reach. You can also make efficient use of space since when you pop something off the stack you free that memory back up.

Every time you write a program, the computer gives you a piece of scratch paper in the form of a stack. Every variable you declare gets pushed to the stack, although Objects behave a little differently than primitive types (more on that in a little bit). 

Every time you call a method, the computer gives you a new little sub-stack so that method can have its own scoped scratch paper. This process allows scoping and keeps memory manageable. Every time you call a method, it makes a new little piece of scratch paper, when the method finishes it throws that scratch away to keep your stack clean. 

### You said something about Objects Being Different than Primitive Types

Stacks have their place in the world, but not all data is stored in that way. Arrays, hashes, and other types of objects are  often referred to as *Dynamic Variables*. Dynamic Variables get their "dynamism" from being stored in something called the *Heap*. 

While stacks are fast and manage memory well, they have a limited amount of space to work with. This is a tradeoff you make in order to use the organizational structure of a stack.

Remember that everything stored on a computer has a specific memory address, usually shown in hexadecimal notation (0x08FAD...). Whenever you deal with an object you have to call it by its address. If you wanted to insert something sequentially between two addresses in memory, you would have to move literally every memory address that follows after to achieve that result. It would be like trying to add a lot to a highly developed city block. The Heap has features that make dynamically changing the size of variables possible.

## THE HEAP

Heaps can do this because they are allowed to ask the operating system for more memory. Stacks cannot do this because the stack is always growing and retracting, you have to limit its size when you make it to avoid having to move huge chunks of memory.Since every program gets its own stack, it's not completely off base to think of memory as just a bunch of stacks of stacks. If you made every stack gigantic, you would very quickly run out of space in RAM and your computer would run a lot slower. So the size of the stack is about balancing these two colliding issues.

The Heap gets around this issue by not caring about sequencing. If you choose to double the size of an array, for example, the Heap will actually move it to a spot where there is plenty of memory and free up where the array used to be. 

Basically any time you use the 'new' keyword in any language, you are allocating something to the heap. Unlike the stack, memory allocated to the heap is **NOT** automatically deleted. In high level languages such as Ruby, Python and Java, this fact is concealed from the program because those languages have a garbage collector. A garbage collector keeps track of all your heap allocations and deletes them automatically. In lower level languages like C/C++ this has to be done manually, which saves in computation time, but can cause errors if programmers forget to free their variables.

Further it can cause memory to become fragmented. Whereas a stack deletes everything in sequential chunks, the Heap may have to go to many different address to delete all of your objects. Since the Heap keeps track of free memory, if you have many random deletions you have a bunch of tiny free spots in your memory.

### Cache Hits, Pointers and Examples

CPUs at their lowest level can basically only save, store, add and subtract. The process of saving and storing requires basically three arguments: from_where, to_where and how_much. Since a CPU can only load in specific chunk sizes, it helps to have all your data stored in a row.

#### Quick Example of a Stack Call

Say you have ints for height and width, and lets say they each take up 8 bytes. Keep in mind I'm completely making up these numbers, but now let's say the CPU loads 32 bytes at a time. When the CPU loads height it gets 8 bytes it wants and 24 it doesn't, but it still has all 32 bytes stored in its cache. 

Now if you go to load width (and they were declared and stored near each other), the CPU can see that it already has those 8 bytes and it doesn't actually need to load anything off the stack.

Integers in many languages (not Ruby) will be stored on the stack. Since they are relatively small, other variables often get loaded with them. If you happen to be using something in that pack of memory, you get a speed up from the system, this is called a cache hit. Further, the system remembers how big an int is and so you don't have store its size anywhere, just a little bit of code indicating its type. 

#### A Quick Example of a Heap Call

Variables stored on the heap are a different story. Basically the CPU starts out the same way as it would load an int: it gets a memory address and a length to load off the stack. 

The difference is that with the int we were loading that variable's value: some number. With dynamically located variables (stuff on the heap), the values we load here are for ANOTHER SET OF INSTRUCTIONS FOR A LOAD. 

This is called a pointer, and if you ever have any interest in doing system's programming you will be using them explicitly a lot. Pointers are how programs interact with the Heap and also how the system interacts with hardware. Hence the systems programming comment.

Loading from pointers basically means the system has at least 1 extra load to perform, although it can also be quite a few more if you are loading something big.

### Conclusion

Most modern languages have features that prevent programmers from interacting directly with this complicated process. Which can be a great thing since there are a lot of tough to detect errors that can occure from working with pointers directly. Still it's important to understand if you ever have performance concerns.

## References and What I Read

[Stack and Heap overview from LearnCpp.com](http://www.learncpp.com/cpp-tutorial/79-the-stack-and-the-heap/)

[Stack vs Heap from C Bootcamp](http://www.learncpp.com/cpp-tutorial/79-the-stack-and-the-heap/)

[Stack vs Heap Interview Questions](http://www.programmerinterview.com/index.php/data-structures/difference-between-stack-and-heap/)

[This great Stack Overflow Post](http://stackoverflow.com/questions/79923/what-and-where-are-the-stack-and-heap)

[Anatomy of a Program in Memory by Gustavo Duarte](http://duartes.org/gustavo/blog/post/anatomy-of-a-program-in-memory/)

[How Computers Work: The CPU and Memory](http://homepage.cs.uri.edu/faculty/wolfe/book/Readings/Reading04.htm)

[How Computer Memory Works](http://computer.howstuffworks.com/computer-memory.htm)